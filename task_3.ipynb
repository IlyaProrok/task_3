{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Sentiment_analysis-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEs3mOZbol3C",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 Summary\n",
        "\n",
        "In the notebook I used 3 different models: fasttext as a soft baseline, XGBoost as a hard baseline and CNN. \n",
        "\n",
        "As a target metric to compare this models I decided to use ROC-AUC score, since this metric is spesifically relevant for classification tasks where none of the classes has higher priority. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ebJ5sHYdvfc",
        "colab_type": "code",
        "outputId": "007f0cfa-bfcb-402e-c81d-8db4628935fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "pip install fasttext\n",
        "import fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.3)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3021212 sha256=90ea191cd9c326f3a4c6829b4ae0f96cd61e6bb02bcd22c7987edb9e08956352\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRh8M3tuRsbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#some imports\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import bz2\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "import nltk\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXd3gsWe4b-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxmXSqiTRsbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the data and decode it\n",
        "data = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/train.ft.txt.bz2\")\n",
        "data = data.readlines()\n",
        "data = [x.decode('utf-8') for x in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LGEwxcRsbz",
        "colab_type": "text"
      },
      "source": [
        "### Part 1: Soft baseline, fasttext\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTafpxKmuWOj",
        "colab_type": "text"
      },
      "source": [
        "Soft baseline fasttext classification is heavily based on [this](https://www.kaggle.com/ejlok1/fasttext-model-91-7) kernel example with consultation to [official PyPI fasttext documentation](https://pypi.org/project/fasttext/#train_supervised-parameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PfNPEuyslWI",
        "colab_type": "code",
        "outputId": "d70ccf16-e0ca-4328-9ded-22cd03e1b91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "help(fasttext.train_supervised)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function train_supervised in module fasttext.FastText:\n",
            "\n",
            "train_supervised(*kargs, **kwargs)\n",
            "    Train a supervised model and return a model object.\n",
            "    \n",
            "    input must be a filepath. The input text does not need to be tokenized\n",
            "    as per the tokenize function, but it must be preprocessed and encoded\n",
            "    as UTF-8. You might want to consult standard preprocessing scripts such\n",
            "    as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
            "    \n",
            "    The input file must must contain at least one label per line. For an\n",
            "    example consult the example datasets which are part of the fastText\n",
            "    repository such as the dataset pulled by classification-example.sh.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUx6KPrNRsb1",
        "colab_type": "code",
        "outputId": "674187ac-eeba-42f1-a694-41a2f6aa83cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#Building a model \n",
        "model = fasttext.train_supervised('train.txt',label_prefix='__label__', epoch = 10)\n",
        "print(model.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__label__1', '__label__2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-U8reutRsb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test data\n",
        "test = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/test.ft.txt.bz2\")\n",
        "test = test.readlines()\n",
        "test = [x.decode('utf-8') for x in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQUWnAjERscE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing labels from test data\n",
        "test_clear = [i.replace('__label__2 ', '') for i in test]\n",
        "test_clear = [i.replace('__label__1 ', '') for i in test_clear]\n",
        "test_clear = [i.replace('\\n', '') for i in test_clear]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0a70XSwRscM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting the labels of the test set\n",
        "pred = model.predict(test_clear)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5K-wHQhRscW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing '__label__1' to class 0 and '__label__2' to class 1 and predicting the labels\n",
        "labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test]\n",
        "pred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35W9i8WRscc",
        "colab_type": "code",
        "outputId": "e955d542-a515-48e9-e7c8-1ffdf2d791cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Estimating the target quality metric - ROC AUC\n",
        "roc_auc_FT = roc_auc_score(labels, pred_labels)\n",
        "print(\"ROC-AUC for FastText is {}\".format(round(roc_auc_FT,3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC AUC for FastText is 0.917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yLeeD2oRsci",
        "colab_type": "text"
      },
      "source": [
        "### Part 2: Hard baseline: TFIDF + XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-X7ugS6l-1Q",
        "colab_type": "text"
      },
      "source": [
        "Sources of this part of the notebook: \n",
        "\n",
        "1.   tricks for data preparation [from here](https://www.kaggle.com/kevinautin/fully-convolutional-accuracy-94-4-15-min)\n",
        "2.   tricks for tokenization [from here](https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uvdejei5h4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import a smart progress meter\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOgKZmrD_ORZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import train data\n",
        "train = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/train.ft.txt.bz2\")\n",
        "train = train.readlines()\n",
        "train = [x.decode('utf-8') for x in train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow-CxbWUZP7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import test data\n",
        "test = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/test.ft.txt.bz2\")\n",
        "test = test.readlines()\n",
        "test = [x.decode('utf-8') for x in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybF3cywB6mVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for data preproseccing (taken from source 1)\n",
        "def reviewText(review):\n",
        "    review = review.split(' ', 1)[1][:-1].lower()\n",
        "    review = re.sub('\\d','0',review)\n",
        "    if 'www.' in review or 'http:' in review or 'https:' in review or '.com' in review:\n",
        "        review = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", review)\n",
        "    return review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSz3WUU75pIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# second function for processing (inspiration: source 1)\n",
        "def splitReviewsLabels(lines,list = False, review_length = '' ):\n",
        "    '''parameter:\n",
        "    list - desired label output format: \n",
        "      if list = False, output - integer 0 or 1\n",
        "      if list = True, output - list [0,1] or [1,0]\n",
        "    review_length - num of characters in review, by default all characters\n",
        "    '''\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for review in tqdm(lines):\n",
        "        rev = reviewText(review)\n",
        "        if list == True:\n",
        "          label = [1,0] if review.split(' ')[0] == '__label__1' else [0,1]\n",
        "        else:\n",
        "          label = 0 if review.split(' ')[0] == '__label__1' else 1\n",
        "        reviews.append(rev[:review_length])\n",
        "        labels.append(label)\n",
        "    return reviews, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5wt9_QCmttF",
        "colab_type": "code",
        "outputId": "58b6015f-1d14-4a2d-fd64-03459c1a3fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get the data for XGBoost model\n",
        "reviews_train_XGB, y_train_XGB = splitReviewsLabels(train, review_length = 512)\n",
        "reviews_test_XGB, y_test_XGB = splitReviewsLabels(test, review_length = 512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3600000/3600000 [00:45<00:00, 78884.28it/s]\n",
            "100%|██████████| 400000/400000 [00:05<00:00, 78599.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu0QVlQjnVlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I decided to decrease the size of training sample, since I have limited computing power\n",
        "# to do that I made up this stupid function\n",
        "# I used train_test_split to maintain balance \n",
        "def decreaseTrain(X, y, target_size = 1000000):\n",
        "  '''\n",
        "  Input: \n",
        "  X - training data, list\n",
        "  y - labels, list\n",
        "  X and y must be the same size\n",
        "  Parameter:\n",
        "  target_size - target training sample size, (0;len(y))\n",
        "  '''\n",
        "  _, X1, _, Y1 = train_test_split(X, y, test_size=target_size/len(y))\n",
        "  return X1, Y1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLc1LomCG27R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_XGB, Y_train_XGB = decreaseTrain(reviews_train_XGB, y_train_XGB, target_size = 600000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNXenUDe2VAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer taken from source 2\n",
        "def Tokenizer(str_input):\n",
        "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
        "    porter_stemmer=nltk.PorterStemmer()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6eRjNpv6G9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_XGB = TfidfVectorizer(tokenizer=Tokenizer, stop_words='english').fit_transform(X_train_XGB)\n",
        "test_XGB = TfidfVectorizer(tokenizer=Tokenizer, stop_words='english').transform(reviews_test_XGB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agc3CewA6toT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tuning hyperparameters, max tree depth and number of trees\n",
        "model_XGB = XGBClassifier(max_depth=10, n_estimators = 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxI9U19n7J38",
        "colab_type": "code",
        "outputId": "ecc7f5f4-866a-4748-efce-60c0a2a0a6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model_XGB.fit(train_XGB, Y_train_XGB)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cR48UZz7OxF",
        "colab_type": "code",
        "outputId": "3e591041-b116-4afa-cbab-0562d80246a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "roc_auc_XGBoost=(roc_auc_score(y_test_XGB, model_XGB.predict(test_XGB)))\n",
        "print(\"ROC-AUC for XGBoost is {}\".format(round(roc_auc_XGBoost,3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC for XGBoost is 0.788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyyT0Tk6mhsI",
        "colab_type": "text"
      },
      "source": [
        "### Part 3: NN based models\n",
        "\n",
        "NN based model sources:\n",
        "\n",
        "\n",
        "1.   [Source number 1](https://www.kaggle.com/kevinautin/fully-convolutional-accuracy-94-4-15-min)\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3JZYEjw-KDB",
        "colab_type": "code",
        "outputId": "e0649456-617d-4d5a-e876-b426ad123dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get the data with labels in another format \n",
        "reviews_train_NN, y_train_NN = splitReviewsLabels(train, list=True, review_length = 512)\n",
        "reviews_test_NN, y_test_NN = splitReviewsLabels(test, list=True, review_length = 512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3600000/3600000 [00:54<00:00, 65518.48it/s]\n",
            "100%|██████████| 400000/400000 [00:05<00:00, 78249.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvlRdjs8mZ-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_NN = np.array(y_train_NN)\n",
        "y_test_NN = np.array(y_test_NN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVCOgcyMdF7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_NN, Y_train_NN= decreaseTrain(reviews_train_NN, y_train_NN, target_size = 600000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKvodTp3CvPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV4lxYLDHpeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000 #length of vocab\n",
        "maxlen = 128 #max number of words in a review\n",
        "embed_size = 64 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM4SRkTBB2vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(X_train_NN)\n",
        "token_train = tokenizer.texts_to_sequences(X_train_NN)\n",
        "token_test = tokenizer.texts_to_sequences(reviews_test_NN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxB0-o-dByG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pad_sequences(token_train, maxlen=maxlen, padding='post')\n",
        "x_test = pad_sequences(token_test, maxlen=maxlen, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucUauCpwWTFB",
        "colab_type": "text"
      },
      "source": [
        "Convolutional NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yq6ja1oBx9K",
        "colab_type": "code",
        "outputId": "51beca65-8dd5-4fc9-e90b-334045e89f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "# constructing a model, convocutional model with batch normalization and dropouts\n",
        "input = Input(shape=(maxlen,))\n",
        "net = Embedding(max_features, embed_size)(input)\n",
        "net = Dropout(0.2)(net)\n",
        "net = BatchNormalization()(net)\n",
        "\n",
        "net = Conv1D(128, 7, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Conv1D(64, 3, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Conv1D(64, 3, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
        "net1 = BatchNormalization()(net)\n",
        "\n",
        "net = Conv1D(2, 1)(net)\n",
        "net = GlobalAveragePooling1D()(net)\n",
        "output = Activation('softmax')(net)\n",
        "model_conv = Model(inputs = input, outputs = output)\n",
        "model_conv.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model_conv.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 128, 64)           640000    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 128, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 128, 32)           14368     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 128, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 128, 32)           3104      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 128, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 128, 32)           3104      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 128, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 128, 32)           3104      \n",
            "_________________________________________________________________\n",
            "conv1d_20 (Conv1D)           (None, 128, 2)            66        \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_4 ( (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 664,386\n",
            "Trainable params: 664,066\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfY6-5LiBx7J",
        "colab_type": "code",
        "outputId": "d25dc6d1-ba50-4533-c55c-0d25968c41d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model_conv.fit(x_train, Y_train_NN, batch_size=1024, epochs=10, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 648000 samples, validate on 72000 samples\n",
            "Epoch 1/10\n",
            "648000/648000 [==============================] - 36s 56us/step - loss: 0.2336 - acc: 0.9041 - val_loss: 0.1913 - val_acc: 0.9285\n",
            "Epoch 2/10\n",
            "648000/648000 [==============================] - 36s 55us/step - loss: 0.1691 - acc: 0.9359 - val_loss: 0.1732 - val_acc: 0.9353\n",
            "Epoch 3/10\n",
            "648000/648000 [==============================] - 36s 55us/step - loss: 0.1488 - acc: 0.9443 - val_loss: 0.1749 - val_acc: 0.9341\n",
            "Epoch 4/10\n",
            "648000/648000 [==============================] - 36s 55us/step - loss: 0.1320 - acc: 0.9511 - val_loss: 0.1950 - val_acc: 0.9254\n",
            "Epoch 5/10\n",
            "648000/648000 [==============================] - 36s 55us/step - loss: 0.1183 - acc: 0.9564 - val_loss: 0.1877 - val_acc: 0.9323\n",
            "Epoch 6/10\n",
            "648000/648000 [==============================] - 36s 55us/step - loss: 0.1054 - acc: 0.9615 - val_loss: 0.2124 - val_acc: 0.9287\n",
            "Epoch 7/10\n",
            "648000/648000 [==============================] - 35s 55us/step - loss: 0.0957 - acc: 0.9647 - val_loss: 0.2011 - val_acc: 0.9311\n",
            "Epoch 8/10\n",
            "648000/648000 [==============================] - 35s 54us/step - loss: 0.0875 - acc: 0.9679 - val_loss: 0.2307 - val_acc: 0.9225\n",
            "Epoch 9/10\n",
            "648000/648000 [==============================] - 35s 54us/step - loss: 0.0804 - acc: 0.9706 - val_loss: 0.2496 - val_acc: 0.9212\n",
            "Epoch 10/10\n",
            "648000/648000 [==============================] - 35s 54us/step - loss: 0.0744 - acc: 0.9727 - val_loss: 0.2368 - val_acc: 0.9210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1316e4f240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8W_Af81BxwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roc_auc_conv=(roc_auc_score(y_test_NN, model_conv.predict(x_test)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVRIo3-vUWzC",
        "colab_type": "code",
        "outputId": "f71c69f9-0dd4-4456-de55-a528c499f7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"ROC-AUC for conv_NN is {}\".format(round(roc_auc_conv,3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC for RNN is 0.977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oc8fKEqWdk0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i74n7EeWObA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(1000000,100))\n",
        "model.add(LSTM(256,return_sequences=True))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(500,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model_conv.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFgDh624Rsdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    sequences = layers.Input(shape=(maximum_length,))\n",
        "    embedded = layers.Embedding(20000, 64)(sequences)\n",
        "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool1D(3)(x)\n",
        "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool1D(5)(x)\n",
        "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
        "    x = layers.GlobalMaxPool1D()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=sequences, outputs=predictions)\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy']\n",
        "    )\n",
        "    return model\n",
        "    \n",
        "CNN = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c407frEZRsdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CNN.fit(\n",
        "    train_texts, \n",
        "    train_labels, \n",
        "    batch_size=128,\n",
        "    epochs=2,\n",
        "    validation_data=(val_texts, val_labels), )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrUvtcB2Rsdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = CNN.predict(test_texts)\n",
        "roc_auc_CNN = roc_auc_score(test_labels, preds)\n",
        "print('ROC AUC for CNN is', roc_auc_CNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNkR6dLvRsdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_rnn_model():\n",
        "    sequences = layers.Input(shape=(maximum_length,))\n",
        "    embedded = layers.Embedding(20000, 64)(sequences)\n",
        "    x = layers.CuDNNGRU(128, return_sequences=True)(embedded)\n",
        "    x = layers.CuDNNGRU(128)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=sequences, outputs=predictions)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "rnn_model = build_rnn_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Ylij12Rsdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_modelt.fit(\n",
        "    train_texts, \n",
        "    train_labels, \n",
        "    batch_size=128,\n",
        "    epochs=1,\n",
        "    validation_data=(val_texts, val_labels), )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRlceP5kRsdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RNN_preds = rnn_model.predict(test_texts)\n",
        "roc_auc_RNN = roc_auc_score(test_labels, RNN_preds)\n",
        "print('ROC AUC for RNN', roc_auc_RNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3GAOAKNRsdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_LSTM_model():\n",
        "    model=Sequential()\n",
        "    model.add(Embedding(1000000,100))\n",
        "    model.add(LSTM(256,return_sequences=True))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(500,activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(100,activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(2,activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['roc_auc_score']\n",
        "    \n",
        "    return model\n",
        "\n",
        "LSTM_model = build_LSTM_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf3NkjNQRsdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_modelt.fit(\n",
        "    train_texts, \n",
        "    train_labels, \n",
        "    batch_size=128,\n",
        "    epochs=1,\n",
        "    validation_data=(val_texts, val_labels), )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOVS0TcURsd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_preds = LSTM_model.predict(test_texts)\n",
        "print('ROC AUC for LSTM', (roc_auc_score(test_labels, LSTM_preds)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_S0rbTGRsd4",
        "colab_type": "text"
      },
      "source": [
        "Heavily based on: https://www.kaggle.com/ejlok1/fasttext-model-91-7, https://www.kaggle.com/saishan/sentiment-analysis-logregre-vs-cudnnlstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHEuk-dlRsd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}