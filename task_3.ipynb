{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Sentiment_analysis-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEs3mOZbol3C",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 Summary\n",
        "\n",
        "In the notebook I used 3 different models: fasttext as a soft baseline, XGBoost as a hard baseline and CNN. \n",
        "\n",
        "As a target metric to compare this models I decided to use ROC-AUC score, since this metric is spesifically relevant for classification tasks where none of the classes has higher priority. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ebJ5sHYdvfc",
        "colab_type": "code",
        "outputId": "007f0cfa-bfcb-402e-c81d-8db4628935fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "pip install fasttext\n",
        "import fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.3)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3021212 sha256=90ea191cd9c326f3a4c6829b4ae0f96cd61e6bb02bcd22c7987edb9e08956352\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRh8M3tuRsbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#some imports\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import bz2\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "import nltk\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKlWXcX5fUyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "200ccbe9-032e-41a7-bcea-f1776ce2c667"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.3)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=e538f3e1a7b631d5795d87ef456480c13be95446b29d481660533f991a538a4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHpn9YbAe-N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXd3gsWe4b-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "325ca01e-c0af-41a3-e9c1-29cd5b7e3b4f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxmXSqiTRsbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the data and decode it\n",
        "data = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/train.ft.txt.bz2\")\n",
        "data = data.readlines()\n",
        "data = [x.decode('utf-8') for x in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LGEwxcRsbz",
        "colab_type": "text"
      },
      "source": [
        "### Part 1: Soft baseline, fasttext\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTafpxKmuWOj",
        "colab_type": "text"
      },
      "source": [
        "Soft baseline fasttext classification is heavily based on [this](https://www.kaggle.com/ejlok1/fasttext-model-91-7) kernel example with consultation to [official PyPI fasttext documentation](https://pypi.org/project/fasttext/#train_supervised-parameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PfNPEuyslWI",
        "colab_type": "code",
        "outputId": "d70ccf16-e0ca-4328-9ded-22cd03e1b91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "help(fasttext.train_supervised)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function train_supervised in module fasttext.FastText:\n",
            "\n",
            "train_supervised(*kargs, **kwargs)\n",
            "    Train a supervised model and return a model object.\n",
            "    \n",
            "    input must be a filepath. The input text does not need to be tokenized\n",
            "    as per the tokenize function, but it must be preprocessed and encoded\n",
            "    as UTF-8. You might want to consult standard preprocessing scripts such\n",
            "    as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
            "    \n",
            "    The input file must must contain at least one label per line. For an\n",
            "    example consult the example datasets which are part of the fastText\n",
            "    repository such as the dataset pulled by classification-example.sh.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUx6KPrNRsb1",
        "colab_type": "code",
        "outputId": "674187ac-eeba-42f1-a694-41a2f6aa83cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#Building a model \n",
        "model = fasttext.train_supervised('train.txt',label_prefix='__label__', epoch = 10)\n",
        "print(model.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__label__1', '__label__2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-U8reutRsb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test data\n",
        "test = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/test.ft.txt.bz2\")\n",
        "test = test.readlines()\n",
        "test = [x.decode('utf-8') for x in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQUWnAjERscE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing labels from test data\n",
        "test_clear = [i.replace('__label__2 ', '') for i in test]\n",
        "test_clear = [i.replace('__label__1 ', '') for i in test_clear]\n",
        "test_clear = [i.replace('\\n', '') for i in test_clear]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0a70XSwRscM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting the labels of the test set\n",
        "pred = model.predict(test_clear)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5K-wHQhRscW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test]\n",
        "pred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35W9i8WRscc",
        "colab_type": "code",
        "outputId": "e955d542-a515-48e9-e7c8-1ffdf2d791cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "roc_auc_FT = roc_auc_score(labels, pred_labels)\n",
        "print(\"ROC-AUC for FastText is {}\".format(round(roc_auc_FT,3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC AUC for FastText is 0.917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yLeeD2oRsci",
        "colab_type": "text"
      },
      "source": [
        "### Part 2: Hard baseline: TFIDF + XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-X7ugS6l-1Q",
        "colab_type": "text"
      },
      "source": [
        "Sources of this part of the notebook: \n",
        "\n",
        "1.   tricks for data preparation [from here](https://www.kaggle.com/kevinautin/fully-convolutional-accuracy-94-4-15-min)\n",
        "2.   tricks for tokenization [from here](https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uvdejei5h4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import a smart progress meter\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOgKZmrD_ORZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import train data\n",
        "train = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/train.ft.txt.bz2\")\n",
        "train = train.readlines()\n",
        "train = [x.decode('utf-8') for x in train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow-CxbWUZP7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import test data\n",
        "test = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/test.ft.txt.bz2\")\n",
        "test = test.readlines()\n",
        "test = [x.decode('utf-8') for x in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybF3cywB6mVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for data preproseccing (taken from source 1)\n",
        "def reviewText(review):\n",
        "    review = review.split(' ', 1)[1][:-1].lower()\n",
        "    review = re.sub('\\d','0',review)\n",
        "    if 'www.' in review or 'http:' in review or 'https:' in review or '.com' in review:\n",
        "        review = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", review)\n",
        "    return review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSz3WUU75pIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# second function for processing (inspiration: source 1)\n",
        "def splitReviewsLabels(lines,list = False, review_length = '' ):\n",
        "    '''parameter:\n",
        "    list - desired label output format: \n",
        "      if list = False, output - integer 0 or 1\n",
        "      if list = True, output - list [0,1] or [1,0]\n",
        "    review_length - num of characters in review, by default all characters\n",
        "    '''\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for review in tqdm(lines):\n",
        "        rev = reviewText(review)\n",
        "        if list == True:\n",
        "          label = [1,0] if review.split(' ')[0] == '__label__1' else [0,1]\n",
        "        else:\n",
        "          label = 0 if review.split(' ')[0] == '__label__1' else 1\n",
        "        reviews.append(rev[:review_length])\n",
        "        labels.append(label)\n",
        "    return reviews, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5wt9_QCmttF",
        "colab_type": "code",
        "outputId": "eb79c3bd-5b71-41e1-8480-46b5604a0702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get the data for XGBoost model\n",
        "reviews_train_XGB, y_train_XGB = splitReviewsLabels(train, review_length = 512)\n",
        "reviews_test_XGB, y_test_XGB = splitReviewsLabels(test, review_length = 512)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3600000/3600000 [00:41<00:00, 86110.48it/s]\n",
            "100%|██████████| 400000/400000 [00:04<00:00, 85699.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu0QVlQjnVlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I decided to decrease the size of training sample, since I have limited computing power\n",
        "# to do that I made up this stupid function\n",
        "# I used train_test_split to maintain balance \n",
        "def decreaseTrain(X, y, target_size = 1000000):\n",
        "  '''\n",
        "  Input: \n",
        "  X - training data, list\n",
        "  y - labels, list\n",
        "  X and y must be the same size\n",
        "  Parameter:\n",
        "  target_size - target training sample size, (0;len(y))\n",
        "  '''\n",
        "  _, X1, _, Y1 = train_test_split(X, y, test_size=target_size/len(y))\n",
        "  return X1, Y1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLc1LomCG27R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_XGB, Y_train_XGB = decreaseTrain(reviews_train_XGB, y_train_XGB, target_size = 600000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otDLyTJof0PX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train, test, reviews_train_XGB, y_train_XGB, reviews_test_XGB, y_test_XGB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNXenUDe2VAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer taken from source 2\n",
        "def Tokenizer_XGB(str_input):\n",
        "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
        "    porter_stemmer=nltk.PorterStemmer()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtMQYm0SjCAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=Tokenizer_XGB, stop_words='english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6eRjNpv6G9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "620f8fd4-0ab3-4d1e-bf56-663e1726c05a"
      },
      "source": [
        "train_XGB = vectorizer.fit_transform(X_train_XGB)\n",
        "test_XGB = vectorizer.transform(X_test_XGB)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agc3CewA6toT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tuning hyperparameters, max tree depth and number of trees\n",
        "model_XGB = XGBClassifier(max_depth=10, n_estimators = 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxI9U19n7J38",
        "colab_type": "code",
        "outputId": "f1dd05f5-97e9-4551-ddc0-efa3cbedca7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model_XGB.fit(train_XGB, Y_train_XGB)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
              "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cR48UZz7OxF",
        "colab_type": "code",
        "outputId": "c10d8cef-ce11-4603-f204-adf2e847a6f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "roc_auc_XGBoost=(roc_auc_score(Y_test_XGB, model_XGB.predict(test_XGB)))\n",
        "print(\"ROC-AUC for XGBoost is {}\".format(round(roc_auc_XGBoost,3)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC for XGBoost is 0.807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyyT0Tk6mhsI",
        "colab_type": "text"
      },
      "source": [
        "### Part 3: NN based models\n",
        "\n",
        "NN based model sources:\n",
        "\n",
        "\n",
        "1.   [Source number 1](https://www.kaggle.com/kevinautin/fully-convolutional-accuracy-94-4-15-min)\n",
        "2.   [Source number 2](https://medium.com/analytics-vidhya/https-medium-com-understanding-attention-mechanism-natural-language-processing-9744ab6aed6a)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj4iJ2Vvv2Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import train data\n",
        "train = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/train.ft.txt.bz2\")\n",
        "train = train.readlines()\n",
        "train = [x.decode('utf-8') for x in train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlfZBcPyv44z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import test data\n",
        "test = bz2.BZ2File(\"../content/drive/My Drive/amazonreviews/test.ft.txt.bz2\")\n",
        "test = test.readlines()\n",
        "test = [x.decode('utf-8') for x in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3JZYEjw-KDB",
        "colab_type": "code",
        "outputId": "6b5834e4-73d7-40ca-e63b-42740b0870ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get the data with labels in another format \n",
        "reviews_train_NN, y_train_NN = splitReviewsLabels(train, list=True, review_length = 512)\n",
        "reviews_test_NN, y_test_NN = splitReviewsLabels(test, list=True, review_length = 512)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3600000/3600000 [00:46<00:00, 78256.42it/s]\n",
            "100%|██████████| 400000/400000 [00:05<00:00, 73311.92it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvlRdjs8mZ-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_NN = np.array(y_train_NN)\n",
        "y_test_NN = np.array(y_test_NN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVCOgcyMdF7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_NN, Y_train_NN= decreaseTrain(reviews_train_NN, y_train_NN, target_size = 600000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKvodTp3CvPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train, test, reviews_train_NN, y_train_NN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV4lxYLDHpeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000 #length of vocab\n",
        "maxlen = 128 #max number of words in a review\n",
        "embed_size = 64 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FRewpKiw6sO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.preprocessing.text import Tokenizer as Tokenizer1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM4SRkTBB2vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer1(num_words=max_features)\n",
        "tokenizer.fit_on_texts(X_train_NN)\n",
        "token_train = tokenizer.texts_to_sequences(X_train_NN)\n",
        "token_test = tokenizer.texts_to_sequences(reviews_test_NN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxB0-o-dByG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pad_sequences(token_train, maxlen=maxlen, padding='post')\n",
        "x_test = pad_sequences(token_test, maxlen=maxlen, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucUauCpwWTFB",
        "colab_type": "text"
      },
      "source": [
        "Convolutional NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yq6ja1oBx9K",
        "colab_type": "code",
        "outputId": "60dd0f27-166e-40b7-f25c-5a95ddd2e977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "# constructing a model, convocutional model with batch normalization and dropouts\n",
        "input = Input(shape=(maxlen,))\n",
        "net = Embedding(max_features, embed_size)(input)\n",
        "net = Dropout(0.2)(net)\n",
        "net = BatchNormalization()(net)\n",
        "\n",
        "net = Conv1D(128, 7, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Conv1D(64, 3, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Conv1D(64, 3, padding='same', activation='relu')(net)\n",
        "net = BatchNormalization()(net)\n",
        "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
        "net1 = BatchNormalization()(net)\n",
        "\n",
        "net = Conv1D(2, 1)(net)\n",
        "net = GlobalAveragePooling1D()(net)\n",
        "output = Activation('softmax')(net)\n",
        "model_conv = Model(inputs = input, outputs = output)\n",
        "model_conv.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model_conv.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 128, 64)           640000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 128, 128)          57472     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128, 128)          512       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 128, 64)           24640     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 128, 64)           12352     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 128, 32)           6176      \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 128, 2)            66        \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 741,986\n",
            "Trainable params: 741,346\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfY6-5LiBx7J",
        "colab_type": "code",
        "outputId": "e39b1f85-33e3-4d1c-8f30-0d74bcc6b7d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model_conv.fit(x_train, Y_train_NN, batch_size=1024, epochs=10, validation_split=0.1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 540000 samples, validate on 60000 samples\n",
            "Epoch 1/10\n",
            "540000/540000 [==============================] - 28s 52us/step - loss: 0.2355 - acc: 0.9030 - val_loss: 0.2751 - val_acc: 0.8843\n",
            "Epoch 2/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.1696 - acc: 0.9353 - val_loss: 0.1833 - val_acc: 0.9318\n",
            "Epoch 3/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.1468 - acc: 0.9451 - val_loss: 0.1747 - val_acc: 0.9352\n",
            "Epoch 4/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.1278 - acc: 0.9527 - val_loss: 0.1905 - val_acc: 0.9313\n",
            "Epoch 5/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.1105 - acc: 0.9597 - val_loss: 0.1938 - val_acc: 0.9324\n",
            "Epoch 6/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.0958 - acc: 0.9654 - val_loss: 0.2417 - val_acc: 0.9188\n",
            "Epoch 7/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.0846 - acc: 0.9693 - val_loss: 0.2068 - val_acc: 0.9273\n",
            "Epoch 8/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.0741 - acc: 0.9731 - val_loss: 0.2404 - val_acc: 0.9283\n",
            "Epoch 9/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.0658 - acc: 0.9762 - val_loss: 0.2580 - val_acc: 0.9250\n",
            "Epoch 10/10\n",
            "540000/540000 [==============================] - 22s 41us/step - loss: 0.0611 - acc: 0.9778 - val_loss: 0.2533 - val_acc: 0.9273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbfab596780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVRIo3-vUWzC",
        "colab_type": "code",
        "outputId": "5d4e3fe3-8ab5-4c04-a2d6-012d56d2c207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "roc_auc_conv=(roc_auc_score(y_test_NN, model_conv.predict(x_test)))\n",
        "print(\"ROC-AUC for conv_NN is {}\".format(round(roc_auc_conv,3)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC for conv_NN is 0.977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oc8fKEqWdk0",
        "colab_type": "text"
      },
      "source": [
        "Self attention model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOgQse7iZoO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5f087b07-a8f3-4e3d-a373-37214278168f"
      },
      "source": [
        "# model structure inspired by source 2\n",
        "# the model has RNN and self attention layers\n",
        "model_att=Sequential()\n",
        "model_att.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
        "model_att.add(Bidirectional(LSTM(units = 16, return_sequences = True, dropout = 0.5, recurrent_dropout = 0.7)))\n",
        "model_att.add(SeqSelfAttention(attention_activation = 'sigmoid'))\n",
        "model_att.add(Flatten())\n",
        "model_att.add(Dense(2, activation = 'softmax'))\n",
        "model_att.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model_att.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 128, 64)           640000    \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 128, 32)           10368     \n",
            "_________________________________________________________________\n",
            "seq_self_attention_3 (SeqSel (None, 128, 32)           2113      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 660,675\n",
            "Trainable params: 660,675\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXk-sXytbSdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "34ed1f07-c1f0-4156-e7b3-f5d6f129a13d"
      },
      "source": [
        "model_att.fit(x_train, Y_train_NN, batch_size=1024, epochs=10, validation_split=0.1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 540000 samples, validate on 60000 samples\n",
            "Epoch 1/10\n",
            "540000/540000 [==============================] - 288s 533us/step - loss: 0.3464 - acc: 0.8393 - val_loss: 0.2471 - val_acc: 0.9006\n",
            "Epoch 2/10\n",
            "540000/540000 [==============================] - 285s 528us/step - loss: 0.2371 - acc: 0.9064 - val_loss: 0.2234 - val_acc: 0.9129\n",
            "Epoch 3/10\n",
            "540000/540000 [==============================] - 285s 527us/step - loss: 0.2141 - acc: 0.9165 - val_loss: 0.2051 - val_acc: 0.9215\n",
            "Epoch 4/10\n",
            "540000/540000 [==============================] - 283s 525us/step - loss: 0.1974 - acc: 0.9237 - val_loss: 0.1987 - val_acc: 0.9244\n",
            "Epoch 5/10\n",
            "540000/540000 [==============================] - 285s 528us/step - loss: 0.1856 - acc: 0.9286 - val_loss: 0.1951 - val_acc: 0.9244\n",
            "Epoch 6/10\n",
            "540000/540000 [==============================] - 284s 526us/step - loss: 0.1795 - acc: 0.9312 - val_loss: 0.1917 - val_acc: 0.9267\n",
            "Epoch 7/10\n",
            "540000/540000 [==============================] - 285s 527us/step - loss: 0.1735 - acc: 0.9339 - val_loss: 0.1895 - val_acc: 0.9276\n",
            "Epoch 8/10\n",
            "540000/540000 [==============================] - 283s 524us/step - loss: 0.1690 - acc: 0.9356 - val_loss: 0.1879 - val_acc: 0.9293\n",
            "Epoch 9/10\n",
            "540000/540000 [==============================] - 285s 528us/step - loss: 0.1655 - acc: 0.9372 - val_loss: 0.1880 - val_acc: 0.9302\n",
            "Epoch 10/10\n",
            "540000/540000 [==============================] - 285s 528us/step - loss: 0.1623 - acc: 0.9385 - val_loss: 0.1875 - val_acc: 0.9294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbfaf129a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYNqvTtz_Yo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9573bca8-29b8-4a4f-e8ab-2a83827f8d87"
      },
      "source": [
        "roc_auc_att=(roc_auc_score(y_test_NN, model_att.predict(x_test)))\n",
        "print(\"ROC-AUC for attention_NN is {}\".format(round(roc_auc_att,3)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC for attention_NN is 0.979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW7hqvVV7cMB",
        "colab_type": "text"
      },
      "source": [
        "Results of the experiment is following: \n",
        "\n",
        "\n",
        "*   ROC-AUC for FastText is 0.917\n",
        "*   ROC-AUC for XGBoost is 0.807\n",
        "*   ROC-AUC for conv_NN is 0.977\n",
        "*   ROC-AUC for attention_NN is 0.979\n",
        "\n",
        "The best ROC-AUC score is given by convolutional NN and attention RNN. Both NNs look quite promising and probably they would give better results if we let it learn all the evailable information."
      ]
    }
  ]
}